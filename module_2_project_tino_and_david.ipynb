{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainy Days and Mondays Always Get Them Down\n",
    "## Data Science Module 2 Final Lab\n",
    "#### Tino Pietrassyk & David Haase (NYC-MHTN-DS-042219)\n",
    "Typically, when you are writing something that's technical, you want a headline that's appealing. This will help you to reach a wider audience and let your work have more visibility and impact as well. The headline or topics of your tutorial should include a short title (typically 10~15 maximum words). The title can include the technology and technique you are talking about, combined with a nice action verb that appeals to a wider audience. For instance, I titled one of my articles as follows: \"Building RNNs is Fun with PyTorch and Google Colab\". You can get pretty creative with the headline, but keep in mind that this is the face of your article and it should be given plenty of tinkering before you finalize it.\n",
    "\n",
    "## Scraping Weather & Sports Data into MongoDB\n",
    "The project description marks the beginning of the tutorial you are writing. It should be clear, concise, and interesting. Here I suggest you to briefly explain what the following notebook tutorial does (usually one sentence)? Then you can explain what technologies you will be using in the tutorial (usually one sentence)? You can also briefly explain what value or knowledge the user will obtain after finishing the tutorial (a short list or two sentences will do)? In addition, you can give credit to any of the notable resources you are utilizing, and also briefly introduce yourself if the project description is not too lengthy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import weathergetter as wg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources & Credits\n",
    "#### Dark Sky\n",
    "Historical weather by date and locationImplemented with the free version of **Dark Sky API** (https://darksky.net/poweredby/)\n",
    "#### Google Maps\n",
    "Implemented with free version of Geocoding from Google's Places APIs\n",
    "\n",
    "#### Project Template\n",
    "Written with ❤️ by dair.ai\n",
    "\n",
    "## [Data Loading]\n",
    "The first step of the pipeline with any data science related tutorial is usually the data loading component. Besides visually describing the dataset in use to your audience, also try to briefly explain (in one or two sentences) where the data came from, i.e., the source of the data. Other specifications like dimensions and attribute type are important but can be neatly explained with examples using code and tools such as pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Data Preprocessing]\n",
    "Although sometimes not necessary, as some datasets already come preprocessed, I believe it is important to slightly mention what type of preprocessing steps the data has undergone -- even if you need to do this through code examples. It should clarify any confusion that can present itself during the modeling section of the tutorial. Remember, your audience wants to get a broad understanding of the data before the modeling component of the tutorial, so try to explain this part of the tutorial as clear as possible with examples. Take advantage of your notebook features and other tools such as matplotlib and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCATION ERROR] Unable to find coordinates for sajdflk;asjl;df. Using Berlin\n",
      "WEATHER SUMMARY for Berlin: Partly cloudy throughout the day.\n",
      "WEATHER ICON for Berlin: partly-cloudy-day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.WeatherGetter().is_rain('07/10/1967', 'sajdflk;asjl;df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Writing to the Database]\n",
    "We are using our local instance of mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONNECT -- build a connection to the local instance of MongoDB\n",
    "address = 'mongodb://127.0.0.1:27017/'\n",
    "db_name = 'euro_football'\n",
    "collection_name = 'year_2011'\n",
    "\n",
    "myclient = pymongo.MongoClient(address)\n",
    "db = myclient[db_name]\n",
    "mycollection = db[collection_name]\n",
    "\n",
    "# Scratch test with dummy CLASS\n",
    "class Team:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def to_json(self):\n",
    "        ret_val = {}\n",
    "        ret_val['name'] = self.name\n",
    "        return ret_val\n",
    "\n",
    "d = Team('David')\n",
    "t = Team('Tino')\n",
    "teams = [d,t]\n",
    "\n",
    "# INSERT -- try to inserte a record for each team\n",
    "try:\n",
    "    insertion_results = mycollection.insert_many([team.to_json() for team in teams])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# SUM-CHECK -- compare results of insertion\n",
    "print(f'{len(insertion_results.inserted_ids)} teams out of {len(teams)} inserted into {db_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Testing Model]\n",
    "One of the things I have learned over the years is that everything in data science is better understood with examples, rather than just using plain code or pictures. Before you begin training your models make sure to explain to the reader what the model is expecting as input and what it is expected to output. Rendering code here with nice descriptions help to prepare the reader on what to expect during training the model, especially since the training code is usually longer than most sections of the tutorial. With libraries like PyTorch and DyNet this is fairly easy since they are dynamic computing libraries. TensorFlow also offers an eager execution command, tf.enable_eager_execution() to evaluate operations immediately. This is what's called imperative programming and I am glad they have it. It makes it easy to teach others about the beautiful things these tools are able to accomplish. I like to think that data science is about storytelling and discovery, and it should remain that way. Clear writing helps!\n",
    "## [Training Model]\n",
    "When training the models you would specify what kind of optimization, hyperparameters, and data iterating methods you are using. To be honest, the training code is usually self-explanatory. If you did your job at the beginning, explaining your dataset and testing the model, this part of the tutorial is probably the one that needs less explanation. In my experience, most data computing libraries use similar training strategies, thus the training structure has become ubiquitous in some sense. If there is still any clarification in your training that you need the reader to know, you can always explain it beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Evaluating Model]\n",
    "And lastly, it is good practice to evaluate your models on some held out samples of the dataset. This helps the reader to get a gist of what the tutorial you just showed him/her contains. It also helps to re-emphasize on the values the tutorial is providing for the reader. This part of the tutorial also helps to finalize your final thoughts and share insights with your readers. Readers love insights. You can share plots, a lot of examples, and even explore the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Final Words]\n",
    "You are not writing a book, so it is not necessary to have a conclusion section. In my experience, you use the final section to summarize all your findings and the future ideas you are working on. This is also a great time to congratualte the reader for making it to the end of the tutorial -- that's a huge achievement. You show that you appreciate the readers. Then you can end the section with your favorite quote.\n",
    "\n",
    "And that's it! Congratulations for reaching the end of this primer. You are now more than equipped to deliver excellent tutorials to the whole data science community and to a wider audience. With this short primer, you should reach thousands, and hopefully millions, but most importantly, with it, you should be able to bring value to your readers and keep expanding the human knowledge base.\n",
    "\n",
    "## [References]\n",
    "Remember to always give credit where it is due. It shows you are responsible and care for the long-term success of the community. Papers, other implementations, video, code repositories, etc., are some of the things are you looking to reference. If you don't want to include this very formal reference section, make sure to embed links throughout the tutorial as an alternative.\n",
    "\n",
    "Written with ❤️ by dair.ai\n",
    "\n",
    "## [Other Tips]\n",
    "Try to ensure that your notebook-based tutorials have a very nice flow. If you are using a lot of functions, it will be nice if you can create seperate python files for them and import them here. You don't want your notebooks to be too detailed, but you also don't want it to be too flat.\n",
    "Remember! You are teaching not dictating. Ask questions and immerse the reader, challenge them. There are various ways to do so.\n",
    "More coming soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
